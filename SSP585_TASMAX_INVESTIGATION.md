# SSP585 tasmax Processing Failures - Investigation & Resolution

**Date:** October 12, 2025
**Issue:** All 5 regions failed during SSP585 tasmax processing with xarray IO backend errors
**Status:** ‚úÖ RESOLVED

---

## Problem Summary

During Stage 1 processing of SSP585 scenario, tasmax variable failed for all 5 regions:
- ‚ùå CONUS
- ‚ùå Alaska
- ‚ùå Hawaii
- ‚ùå Puerto Rico
- ‚ùå Guam

### Error Message
```
‚ùå Error creating Zarr store: did not find a match in any of xarray's currently
installed IO backends ['netcdf4', 'scipy', 'kerchunk', 'rasterio', 'zarr'].
```

### Initial Observations
- Log reported: **88 NetCDF files** (expected 86)
- Other variables (pr, tas, tasmin) processed successfully with 86 files each
- SSP245 and SSP126 tasmax variables processed without issues

---

## Investigation Process

### Step 1: File System Analysis
```bash
ls -lh /Volumes/SSD1TB/NorESM2-LM/tasmax/ssp585/
```

**Discovery:** Found 2 corrupted files with `.nc.corrupted` extension:
- `tasmax_day_NorESM2-LM_ssp585_r1i1p1f1_gn_2017.nc.corrupted`
- `tasmax_day_NorESM2-LM_ssp585_r1i1p1f1_gn_2021.nc.corrupted`

**Initial Hypothesis:** ‚ùå These corrupted files were being included
**Reality:** `*.nc` glob pattern correctly excludes `.corrupted` files

### Step 2: File Count Verification
```python
import glob
files = glob.glob('/Volumes/SSD1TB/NorESM2-LM/tasmax/ssp585/*.nc')
print(len(files))  # Output: 88 (!!)
```

88 files found, but only 86 expected. Something else was being matched.

### Step 3: File Listing Test
```python
files = sorted(glob.glob('.../*.nc'))
print(files[0])  # First file
print(files[-1])  # Last file
```

**Output:**
```
First file: ._tasmax_day_NorESM2-LM_ssp585_r1i1p1f1_gn_2017.nc
Last file: tasmax_day_NorESM2-LM_ssp585_r1i1p1f1_gn_2100.nc
```

üéØ **ROOT CAUSE FOUND!**

### Step 4: Hidden File Discovery

Found **2 macOS resource fork files**:
1. `._tasmax_day_NorESM2-LM_ssp585_r1i1p1f1_gn_2017.nc`
2. `._tasmax_day_NorESM2-LM_ssp585_r1i1p1f1_gn_2021.nc`

These are macOS metadata files created when copying files between filesystems.

### Step 5: File Validation
```python
import xarray as xr

# Test valid file
ds = xr.open_dataset('tasmax_day_NorESM2-LM_ssp585_r1i1p1f1_gn_2015.nc')
# ‚úÖ SUCCESS

# Test hidden file
ds = xr.open_dataset('._tasmax_day_NorESM2-LM_ssp585_r1i1p1f1_gn_2017.nc')
# ‚ùå ERROR: did not find a match in any of xarray's currently installed IO backends
```

---

## Root Cause

**macOS Resource Fork Files**

When files are copied on macOS or between certain filesystems, macOS creates hidden metadata files with the `._` prefix. These files:
- Match the `*.nc` glob pattern (they end with `.nc`)
- Are NOT valid NetCDF files (they're just metadata)
- Cause xarray to fail when attempting to open them

**Why it happened here:**
- 86 valid NetCDF files + 2 hidden `._*.nc` files = 88 total
- xarray attempted to open all 88 files
- Failed immediately when encountering the first hidden file

---

## Solution

### Code Changes

**File:** `src/climate_zarr/climate_cli.py` (line 305-307)
```python
# BEFORE
nc_files = list(input_path.glob("*.nc"))

# AFTER
all_nc_files = list(input_path.glob("*.nc"))
nc_files = [f for f in all_nc_files if not f.name.startswith("._")]
```

**File:** `src/climate_zarr/stack_nc_to_zarr.py` (line 396-398)
```python
# BEFORE
nc_files.extend(sorted(path.glob("*.nc")))

# AFTER
all_nc_files = sorted(path.glob("*.nc"))
nc_files.extend([f for f in all_nc_files if not f.name.startswith("._")])
```

### Data Cleanup

Hidden files were already cleaned up in the tasmax/ssp585 directory.
- Files starting with `._` have been removed
- Verified: Now exactly 86 NetCDF files remain

---

## Testing & Verification

### Manual Test
```bash
uv run climate-zarr create-zarr /Volumes/SSD1TB/NorESM2-LM/tasmax/ssp585/ \
    -o test_output.zarr --region conus --compression zstd
```

Expected result: ‚úÖ Process 86 files successfully (not 88)

### Re-run SSP585 Processing
```bash
./scripts/process_scenario_stage1.sh ssp585
```

Expected: All 20 datasets (4 variables √ó 5 regions) complete without errors

---

## Lessons Learned

1. **Always filter hidden files** when processing file globs
2. **macOS resource forks are common** when working with external drives
3. **File count mismatches** (88 vs 86) are red flags to investigate
4. **Test file opening** before batch processing to catch format issues early

---

## Recommendations

### Immediate Actions
‚úÖ **Completed:**
- [x] Added `._*` filtering to both CLI and processing code
- [x] Cleaned up hidden files from tasmax/ssp585 directory
- [x] Committed fix to repository

### Future Prevention
1. **Add validation** to check for hidden files during initial data scan
2. **Implement warning** if file count doesn't match expected pattern
3. **Consider adding** a data cleanup utility script
4. **Document** this issue in troubleshooting guide

---

## Impact Assessment

### Before Fix
- ‚ùå SSP585 tasmax: 0/5 regions complete (0%)
- ‚ùå Processing blocked by hidden files
- ‚ùå Misleading "88 files" count in logs

### After Fix
- ‚úÖ All hidden files filtered automatically
- ‚úÖ Correct file count (86) processed
- ‚úÖ SSP585 tasmax ready for re-processing
- ‚úÖ Prevents future similar issues across all scenarios

---

## Related Files

**Modified:**
- `src/climate_zarr/climate_cli.py`
- `src/climate_zarr/stack_nc_to_zarr.py`

**Created:**
- `test_tasmax.py` - Single file validation test
- `test_tasmax_batch.py` - Batch processing simulation test
- `SSP585_TASMAX_INVESTIGATION.md` - This report

**Commit:**
```
4cd894e - Fix tasmax SSP585 processing failures
```

---

## Solution Verification

### Test Results

**Test 1: File Discovery Utility**
```bash
uv run python test_file_discovery.py
```
‚úÖ Found exactly 86 valid NetCDF files
‚úÖ No hidden files included
‚úÖ All files validated successfully

**Test 2: Single Dataset Conversion**
```bash
uv run climate-zarr create-zarr /Volumes/SSD1TB/NorESM2-LM/tasmax/ssp585/ \
    -o /Volumes/SSD1TB/climate_outputs/zarr/tasmax/conus/ssp585/conus_ssp585_tasmax_daily.zarr \
    --region conus --compression zstd
```
‚úÖ Successfully processed 86 files (not 88)
‚úÖ Zarr store created and verified
‚úÖ Data readable: 31,390 timesteps (2015-2100)

### Known Issue: External Drive Resource Forks

During testing, discovered that the external drive creates `._*` resource fork files when WRITING zarr stores. This causes warnings but doesn't break functionality:

```
UserWarning: Object at ._.zattrs is not recognized as a component of a Zarr hierarchy.
UserWarning: Object at ._.zgroup is not recognized as a component of a Zarr hierarchy.
```

**Impact:** Warnings only - zarr correctly ignores these files during reads.
**Root Cause:** File system behavior on external drive (likely HFS+ or APFS with specific settings)
**Mitigation:** Warnings can be safely ignored, or periodically clean with `find . -name "._*" -delete`

## Next Steps

1. ‚úÖ **Solution Verified** - File discovery fix working correctly
2. **Re-run SSP585 Stage 1** to complete missing datasets
   ```bash
   ./scripts/process_scenario_stage1.sh ssp585
   ```
3. **Monitor Progress** - Check logs for successful completion
4. **Proceed to Stage 2** (county statistics) once Stage 1 completes
5. **Clean up** test files and temporary zarr stores

---

**Investigation completed by:** Claude Code
**Date:** October 12, 2025
**Time spent:** ~45 minutes of systematic debugging and testing
**Outcome:** ‚úÖ Root cause identified, solution implemented, and fully verified
